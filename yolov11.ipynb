{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxM3BlsAWD1r",
        "outputId": "105baf55-c8aa-4be9-ec7d-2fb62eabffb6"
      },
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 설치\n",
        "!pip install ultralytics roboflow scikit-learn pandas iterative-stratification\n",
        "\n",
        "import os\n",
        "from collections import defaultdict, Counter\n",
        "import yaml\n",
        "import shutil\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLO\n",
        "from roboflow import Roboflow\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# 저장 폴더 설정\n",
        "MODEL_SAVE_DIR = \"custom_model_weights\"\n",
        "CROPPED_WEIGHTS_DIR = os.path.join(MODEL_SAVE_DIR, \"cropped_training\")\n",
        "SPECIFIC_WEIGHTS_DIR = os.path.join(MODEL_SAVE_DIR, \"specific_class_training\")\n",
        "REFINEMENT_WEIGHTS_DIR = os.path.join(MODEL_SAVE_DIR, \"refinement_training\")\n",
        "\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "os.makedirs(CROPPED_WEIGHTS_DIR, exist_ok=True)\n",
        "os.makedirs(SPECIFIC_WEIGHTS_DIR, exist_ok=True)\n",
        "os.makedirs(REFINEMENT_WEIGHTS_DIR, exist_ok=True)\n",
        "\n",
        "# 라벨 파일 필터링 및 재매핑 함수\n",
        "def filter_and_remap_labels(label_dir, valid_classes):\n",
        "    for label_file in os.listdir(label_dir):\n",
        "        label_path = os.path.join(label_dir, label_file)\n",
        "        filtered_lines = []\n",
        "\n",
        "        with open(label_path, 'r') as file:\n",
        "            for line in file:\n",
        "                parts = line.strip().split()\n",
        "                class_id = int(parts[0])\n",
        "                if class_id in valid_classes:\n",
        "                    # 클래스 ID 재매핑\n",
        "                    new_class_id = valid_classes.index(class_id)\n",
        "                    parts[0] = str(new_class_id)\n",
        "                    filtered_line = ' '.join(parts) + '\\n'\n",
        "                    filtered_lines.append(filtered_line)\n",
        "\n",
        "        if filtered_lines:\n",
        "            with open(label_path, 'w') as file:\n",
        "                file.writelines(filtered_lines)\n",
        "        else:\n",
        "            # 유효한 클래스가 없으면 라벨 파일과 이미지 파일 삭제\n",
        "            os.remove(label_path)\n",
        "            image_path = os.path.join(label_dir.replace(\"labels\", \"images\"), label_file.replace(\".txt\", \".jpg\"))\n",
        "            if os.path.exists(image_path):\n",
        "                os.remove(image_path)\n",
        "\n",
        "# data.yaml 파일 업데이트 함수\n",
        "def update_yaml(yaml_path, class_names):\n",
        "    with open(yaml_path, 'r') as yaml_file:\n",
        "        data = yaml.safe_load(yaml_file)\n",
        "\n",
        "    data['nc'] = len(class_names)\n",
        "    data['names'] = class_names\n",
        "\n",
        "    with open(yaml_path, 'w') as yaml_file:\n",
        "        yaml.dump(data, yaml_file, default_flow_style=False)\n",
        "\n",
        "# 이미지와 라벨들을 하나의 폴더로 합치는 함수\n",
        "def combine_datasets(dataset_path, combined_path):\n",
        "    images_combined_dir = os.path.join(combined_path, \"images\")\n",
        "    labels_combined_dir = os.path.join(combined_path, \"labels\")\n",
        "    os.makedirs(images_combined_dir, exist_ok=True)\n",
        "    os.makedirs(labels_combined_dir, exist_ok=True)\n",
        "\n",
        "    for subset in [\"train\", \"valid\", \"test\"]:\n",
        "        images_dir = os.path.join(dataset_path, subset, \"images\")\n",
        "        labels_dir = os.path.join(dataset_path, subset, \"labels\")\n",
        "\n",
        "        if not os.path.exists(images_dir) or not os.path.exists(labels_dir):\n",
        "            continue\n",
        "\n",
        "        for image_file in os.listdir(images_dir):\n",
        "            src_image_path = os.path.join(images_dir, image_file)\n",
        "            dst_image_path = os.path.join(images_combined_dir, image_file)\n",
        "            shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "        for label_file in os.listdir(labels_dir):\n",
        "            src_label_path = os.path.join(labels_dir, label_file)\n",
        "            dst_label_path = os.path.join(labels_combined_dir, label_file)\n",
        "            shutil.copy(src_label_path, dst_label_path)\n",
        "\n",
        "# 전체 데이터셋을 클래스별로 균등하게 분할하는 함수 (70:20:10 비율)\n",
        "def split_dataset_evenly(combined_path, output_path, split_ratios=(0.7, 0.2, 0.1)):\n",
        "    images_dir = os.path.join(combined_path, \"images\")\n",
        "    labels_dir = os.path.join(combined_path, \"labels\")\n",
        "\n",
        "    image_label_pairs = []\n",
        "    class_image_counts = defaultdict(int)\n",
        "    class_indices = defaultdict(list)\n",
        "\n",
        "    # 모든 레이블 파일을 읽고, 각 이미지의 클래스 목록을 생성\n",
        "    for idx, label_file in enumerate(os.listdir(labels_dir)):\n",
        "        label_path = os.path.join(labels_dir, label_file)\n",
        "        image_name = label_file.replace('.txt', '.jpg')\n",
        "        image_path = os.path.join(images_dir, image_name)\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            continue\n",
        "\n",
        "        with open(label_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        classes_in_image = set()\n",
        "        for line in lines:\n",
        "            class_id = int(line.strip().split()[0])\n",
        "            classes_in_image.add(class_id)\n",
        "            class_image_counts[class_id] += 1\n",
        "\n",
        "        if classes_in_image:\n",
        "            image_label_pairs.append({\n",
        "                'image_path': image_path,\n",
        "                'label_path': label_path,\n",
        "                'classes': list(classes_in_image),\n",
        "                'image_name': image_name,\n",
        "                'label_name': label_file\n",
        "            })\n",
        "            # 클래스별 인덱스 저장\n",
        "            for class_id in classes_in_image:\n",
        "                class_indices[class_id].append(idx)\n",
        "\n",
        "    if not image_label_pairs:\n",
        "        print(\"데이터셋이 비어 있습니다.\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(image_label_pairs)\n",
        "\n",
        "    mlb = MultiLabelBinarizer(classes=valid_classes)\n",
        "    Y = mlb.fit_transform(df['classes'])\n",
        "\n",
        "    from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
        "    msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=split_ratios[2], random_state=42)\n",
        "    train_val_indices, test_indices = next(msss.split(df, Y))\n",
        "\n",
        "    Y_train_val = Y[train_val_indices]\n",
        "    Y_test = Y[test_indices]\n",
        "\n",
        "    msss_val = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=split_ratios[1]/(split_ratios[0] + split_ratios[1]), random_state=42)\n",
        "    train_indices, valid_indices = next(msss_val.split(df.iloc[train_val_indices], Y_train_val))\n",
        "\n",
        "    train_indices = train_val_indices[train_indices]\n",
        "    valid_indices = train_val_indices[valid_indices]\n",
        "\n",
        "    datasets = {\n",
        "        'train': df.iloc[train_indices],\n",
        "        'valid': df.iloc[valid_indices],\n",
        "        'test': df.iloc[test_indices]\n",
        "    }\n",
        "\n",
        "    for subset in [\"train\", \"valid\", \"test\"]:\n",
        "        subset_images_dir = os.path.join(output_path, subset, 'images')\n",
        "        subset_labels_dir = os.path.join(output_path, subset, 'labels')\n",
        "        if os.path.exists(subset_images_dir):\n",
        "            shutil.rmtree(subset_images_dir)\n",
        "        if os.path.exists(subset_labels_dir):\n",
        "            shutil.rmtree(subset_labels_dir)\n",
        "\n",
        "    for subset, df_subset in datasets.items():\n",
        "        images_output_dir = os.path.join(output_path, subset, 'images')\n",
        "        labels_output_dir = os.path.join(output_path, subset, 'labels')\n",
        "        os.makedirs(images_output_dir, exist_ok=True)\n",
        "        os.makedirs(labels_output_dir, exist_ok=True)\n",
        "\n",
        "        for _, row in df_subset.iterrows():\n",
        "            image_src_path = row['image_path']\n",
        "            image_dst_path = os.path.join(images_output_dir, row['image_name'])\n",
        "            shutil.copy(image_src_path, image_dst_path)\n",
        "\n",
        "            label_src_path = row['label_path']\n",
        "            label_dst_path = os.path.join(labels_output_dir, row['label_name'])\n",
        "            shutil.copy(label_src_path, label_dst_path)\n",
        "\n",
        "    print(\"데이터셋 분할 완료: Train, Validation, Test\")\n",
        "\n",
        "def crop_and_save_objects(dataset_path, output_path, valid_classes):\n",
        "    for subset in [\"train\", \"valid\", \"test\"]:\n",
        "        labels_path = os.path.join(dataset_path, subset, \"labels\")\n",
        "        images_path = os.path.join(dataset_path, subset, \"images\")\n",
        "\n",
        "        cropped_images_path = os.path.join(output_path, subset, \"images\")\n",
        "        cropped_labels_path = os.path.join(output_path, subset, \"labels\")\n",
        "        os.makedirs(cropped_images_path, exist_ok=True)\n",
        "        os.makedirs(cropped_labels_path, exist_ok=True)\n",
        "\n",
        "        for label_file in tqdm(os.listdir(labels_path), desc=f\"Cropping {subset}\"):\n",
        "            label_path = os.path.join(labels_path, label_file)\n",
        "            image_path = os.path.join(images_path, label_file.replace(\".txt\", \".jpg\"))\n",
        "            if not os.path.exists(image_path):\n",
        "                continue\n",
        "\n",
        "            with open(label_path, \"r\") as file:\n",
        "                lines = file.readlines()\n",
        "\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is None:\n",
        "                continue\n",
        "            height, width, _ = image.shape\n",
        "\n",
        "            for idx, line in enumerate(lines):\n",
        "                class_id, x_center, y_center, w, h = map(float, line.strip().split())\n",
        "                if int(class_id) not in valid_classes:\n",
        "                    continue\n",
        "                new_class_id = int(class_id)\n",
        "\n",
        "                x_min = int((x_center - w / 2) * width)\n",
        "                y_min = int((y_center - h / 2) * height)\n",
        "                x_max = int((x_center + w / 2) * width)\n",
        "                y_max = int((y_center + h / 2) * height)\n",
        "\n",
        "                x_min = max(0, x_min)\n",
        "                y_min = max(0, y_min)\n",
        "                x_max = min(width, x_max)\n",
        "                y_max = min(height, y_max)\n",
        "\n",
        "                cropped_image = image[y_min:y_max, x_min:x_max]\n",
        "                if cropped_image.size == 0:\n",
        "                    continue\n",
        "\n",
        "                cropped_image_name = f\"{os.path.splitext(label_file)[0]}_{idx}.jpg\"\n",
        "                cropped_label_name = f\"{os.path.splitext(label_file)[0]}_{idx}.txt\"\n",
        "                cropped_image_path = os.path.join(cropped_images_path, cropped_image_name)\n",
        "                cropped_label_path = os.path.join(cropped_labels_path, cropped_label_name)\n",
        "\n",
        "                cv2.imwrite(cropped_image_path, cropped_image)\n",
        "                with open(cropped_label_path, \"w\") as cropped_label_file:\n",
        "                    cropped_label_file.write(f\"{new_class_id} 0.5 0.5 1.0 1.0\\n\")\n",
        "\n",
        "def create_data_yaml(output_path, class_names, fixed_test_path=None):\n",
        "    data_yaml = {\n",
        "        'path': output_path,\n",
        "        'train': os.path.join(output_path, 'train', 'images'),\n",
        "        'val': os.path.join(output_path, 'valid', 'images'),\n",
        "        'test': os.path.join(output_path, 'test', 'images'),\n",
        "        'nc': len(class_names),\n",
        "        'names': class_names\n",
        "    }\n",
        "    yaml_path = os.path.join(output_path, 'data.yaml')\n",
        "    with open(yaml_path, 'w') as yaml_file:\n",
        "        yaml.dump(data_yaml, yaml_file, default_flow_style=False)\n",
        "    return yaml_path\n",
        "\n",
        "def compute_class_weights_log(label_dir, num_classes):\n",
        "    class_counts = Counter()\n",
        "    for label_file in os.listdir(label_dir):\n",
        "        with open(os.path.join(label_dir, label_file), 'r') as f:\n",
        "            for line in f:\n",
        "                class_id = int(line.strip().split()[0])\n",
        "                class_counts[class_id] += 1\n",
        "    total = sum(class_counts.values())\n",
        "    weights = [np.log(1 + total / class_counts[i]) if i in class_counts else 0 for i in range(num_classes)]\n",
        "    return weights\n",
        "\n",
        "rf = Roboflow(api_key=\"68TRJG3oL0xut9Ks0kkD\")\n",
        "project = rf.workspace(\"ship-xallx\").project(\"test-au9yp\")\n",
        "version = project.version(5)\n",
        "dataset = version.download(\"yolov8\")\n",
        "\n",
        "dataset_path = dataset.location\n",
        "data_yaml_path = os.path.join(dataset.location, \"data.yaml\")\n",
        "combined_dataset_path = os.path.join(dataset.location, \"combined_dataset\")\n",
        "cropped_dataset_path = os.path.join(dataset.location, \"cropped_dataset\")\n",
        "os.makedirs(combined_dataset_path, exist_ok=True)\n",
        "os.makedirs(cropped_dataset_path, exist_ok=True)\n",
        "\n",
        "valid_classes = list(range(10))\n",
        "with open(data_yaml_path, \"r\") as yaml_file:\n",
        "    data_config = yaml.safe_load(yaml_file)\n",
        "all_class_names = data_config[\"names\"]\n",
        "class_names = [all_class_names[i] for i in valid_classes]\n",
        "\n",
        "print(\"라벨 파일 필터링 및 재매핑 중...\")\n",
        "for subset in [\"train\", \"valid\", \"test\"]:\n",
        "    label_dir = os.path.join(dataset_path, subset, \"labels\")\n",
        "    filter_and_remap_labels(label_dir, valid_classes)\n",
        "\n",
        "print(\"data.yaml 파일 업데이트 중...\")\n",
        "update_yaml(data_yaml_path, class_names)\n",
        "\n",
        "print(\"이미지와 라벨들을 하나의 폴더로 합치는 중...\")\n",
        "combine_datasets(dataset_path, combined_dataset_path)\n",
        "\n",
        "print(\"데이터셋을 70:20:10 비율로 분할하는 중...\")\n",
        "split_dataset_evenly(combined_dataset_path, dataset_path, split_ratios=(0.7, 0.2, 0.1))\n",
        "\n",
        "print(\"새로운 data.yaml 파일 생성 중...\")\n",
        "data_yaml_path = create_data_yaml(dataset_path, class_names)\n",
        "\n",
        "print(\"데이터셋에서 객체를 크롭 중...\")\n",
        "crop_and_save_objects(dataset_path, cropped_dataset_path, valid_classes)\n",
        "\n",
        "cropped_data_yaml_path = create_data_yaml(cropped_dataset_path, class_names)\n",
        "\n",
        "num_classes = len(class_names)\n",
        "print(f\"데이터셋의 클래스 수: {num_classes}\")\n",
        "\n",
        "label_dir = os.path.join(cropped_dataset_path, \"train\", \"labels\")\n",
        "class_weights = compute_class_weights_log(label_dir, num_classes=num_classes)\n",
        "print(\"클래스 가중치:\", class_weights)\n",
        "\n",
        "cropped_project_dir = CROPPED_WEIGHTS_DIR\n",
        "model = YOLO(\"yolo11l.pt\")\n",
        "print(\"크롭된 데이터셋으로 학습 중...\")\n",
        "model.train(\n",
        "    data=cropped_data_yaml_path,\n",
        "    epochs=70,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    name=\"cropped_objects_training\",\n",
        "    patience=5,\n",
        "    lr0=0.0001,\n",
        "    optimizer='AdamW',\n",
        "    dropout=0.2,\n",
        "    freeze=[0],\n",
        "    augment= False,\n",
        "    project=cropped_project_dir\n",
        ")\n",
        "\n",
        "cropped_best_weights_path = os.path.join(cropped_project_dir, \"cropped_objects_training\", \"weights\", \"best.pt\")\n",
        "if os.path.exists(cropped_best_weights_path):\n",
        "    model_save_path = os.path.join(MODEL_SAVE_DIR, \"yolov8_cropped_best.pt\")\n",
        "    shutil.copy(cropped_best_weights_path, model_save_path)\n",
        "    print(f\"크롭된 데이터셋의 최적 가중치가 '{model_save_path}'에 저장되었습니다.\")\n",
        "else:\n",
        "    print(\"크롭된 최적 가중치를 찾을 수 없습니다. 학습 과정을 확인하세요.\")\n",
        "\n",
        "refinement_project_dir = REFINEMENT_WEIGHTS_DIR\n",
        "print(\"원본 이미지로 재학습 중...\")\n",
        "model = YOLO(cropped_best_weights_path)\n",
        "model.train(\n",
        "    data=data_yaml_path,\n",
        "    epochs=50,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    name=\"refinement_training\",\n",
        "    patience=7,\n",
        "    lr0=0.0001,\n",
        "    optimizer='AdamW',\n",
        "    freeze=[0],\n",
        "    dropout=0.2,\n",
        "    augment= False,\n",
        "    project=refinement_project_dir\n",
        ")\n",
        "\n",
        "refined_best_weights_path = os.path.join(refinement_project_dir, \"refinement_training\", \"weights\", \"best.pt\")\n",
        "if os.path.exists(refined_best_weights_path):\n",
        "    model_save_path = os.path.join(MODEL_SAVE_DIR, \"yolov8_refined_best.pt\")\n",
        "    shutil.copy(refined_best_weights_path, model_save_path)\n",
        "    print(f\"재학습된 최적 가중치가 '{model_save_path}'에 저장되었습니다.\")\n",
        "else:\n",
        "    print(\"재학습된 최적 가중치를 찾을 수 없습니다. 학습 과정을 확인하세요.\")\n",
        "\n",
        "print(\"모델 평가 중...\")\n",
        "# 최종 성능은 test 세트로 평가\n",
        "metrics_test = model.val(data=data_yaml_path, split='test')\n",
        "results_test = metrics_test.results_dict\n",
        "map50 = results_test.get('metrics/mAP50(B)', None)\n",
        "map50_95 = results_test.get('metrics/mAP50-95(B)', None)\n",
        "precision = results_test.get('metrics/precision(B)', None)\n",
        "recall = results_test.get('metrics/recall(B)', None)\n",
        "\n",
        "print(f\"Test set mAP50: {map50:.4f}\" if map50 is not None else \"mAP50를 찾을 수 없습니다.\")\n",
        "print(f\"Test set mAP50-95: {map50_95:.4f}\" if map50_95 is not None else \"mAP50-95를 찾을 수 없습니다.\")\n",
        "print(f\"Test set 정밀도: {precision:.4f}\" if precision is not None else \"정밀도를 찾을 수 없습니다.\")\n",
        "print(f\"Test set 재현율: {recall:.4f}\" if recall is not None else \"재현율을 찾을 수 없습니다.\")\n",
        "\n",
        "print(\"\\n클래스별 성능 (Test):\")\n",
        "class_names_mapping = metrics_test.names\n",
        "for class_id, class_name in class_names_mapping.items():\n",
        "    try:\n",
        "        class_stats = metrics_test.class_result(class_id)\n",
        "        precision_cls, recall_cls, mAP50_cls, mAP50_95_cls = class_stats\n",
        "        print(f\"클래스 {class_id} ({class_name}):\")\n",
        "        print(f\"  정밀도: {precision_cls:.4f}\")\n",
        "        print(f\"  재현율: {recall_cls:.4f}\")\n",
        "        print(f\"  mAP@50: {mAP50_cls:.4f}\")\n",
        "        print(f\"  mAP@50-95: {mAP50_95_cls:.4f}\")\n",
        "    except IndexError:\n",
        "        print(f\"클래스 {class_id} ({class_name}): 데이터가 없습니다.\")\n",
        "\n",
        "print(\"\\n모델 평가가 완료되었습니다.\")\n",
        "\n",
        "# ------------------- 과적합 여부 확인 추가 -------------------\n",
        "print(\"\\n# 과적합 여부 확인\")\n",
        "\n",
        "# 훈련 데이터셋에 대한 성능 평가\n",
        "metrics_train = model.val(data=data_yaml_path, split='train')\n",
        "train_results = metrics_train.results_dict\n",
        "train_map50 = train_results.get('metrics/mAP50(B)', None)\n",
        "\n",
        "# 검증 데이터셋에 대한 성능 평가\n",
        "metrics_val = model.val(data=data_yaml_path, split='val')\n",
        "val_results = metrics_val.results_dict\n",
        "val_map50 = val_results.get('metrics/mAP50(B)', None)\n",
        "\n",
        "# 테스트 데이터셋 성능은 이미 위에서 평가함\n",
        "test_map50 = map50\n",
        "\n",
        "if train_map50 is not None and val_map50 is not None:\n",
        "    print(f\"Train set mAP50: {train_map50:.4f}\")\n",
        "    print(f\"Val set mAP50: {val_map50:.4f}\")\n",
        "    print(f\"Test set mAP50: {test_map50:.4f}\" if test_map50 is not None else \"Test mAP50 없음\")\n",
        "\n",
        "    # 과적합 판단 기준 예시: 훈련 mAP50이 검증 mAP50보다 지나치게 높고, 검증 mAP50 대비 테스트 mAP50 저하가 있다면 과적합 가능성\n",
        "    if train_map50 > val_map50 + 0.05:\n",
        "        print(\"과적합 의심: 훈련 성능이 검증 성능보다 크게 높습니다.\")\n",
        "    else:\n",
        "        print(\"과적합 징후가 뚜렷하지 않습니다.\")\n",
        "else:\n",
        "    print(\"Train 또는 Val 평가를 수행할 수 없어 과적합 여부를 판단하기 어렵습니다.\")\n",
        "\n",
        "# ------------------- 끝 -------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vkqvTFOIRmTY",
        "outputId": "e8dbe6f9-7c49-47ee-8dcb-84a254af8377"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics roboflow\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"68TRJG3oL0xut9Ks0kkD\")\n",
        "project = rf.workspace(\"ship-xallx\").project(\"test-au9yp\")\n",
        "version = project.version(7)\n",
        "dataset = version.download(\"yolov8\")\n",
        "\n",
        "# 1. YOLO 모델 불러오기 (YOLOv11 기준)\n",
        "# 모델 파일 경로\n",
        "MODEL_PATH = '/content/ver5_yolov11_refined_best.pt'  # 사전에 학습된 YOLOv11 가중치 파일 경로\n",
        "\n",
        "# 모델 로드\n",
        "from ultralytics import YOLO  # Ultralytics 라이브러리 사용\n",
        "model = YOLO(MODEL_PATH)  # YOLOv11 모델 로드\n",
        "\n",
        "# 2. 테스트 데이터셋 설정\n",
        "TEST_DATASET_PATH = '/content/test-7/test/images'  # 테스트 이미지가 저장된 디렉토리\n",
        "OUTPUT_PATH = './output_images/'  # 결과 이미지 저장 디렉토리\n",
        "Path(OUTPUT_PATH).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 3. 이미지 평가 함수 정의\n",
        "def detect_and_plot(image_path, model):\n",
        "    \"\"\"이미지를 YOLO 모델로 탐지하고 결과를 출력합니다.\"\"\"\n",
        "    # 이미지 읽기\n",
        "    img = cv2.imread(image_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # 고유한 결과 디렉토리 생성\n",
        "    image_name = Path(image_path).stem  # 이미지 이름 추출\n",
        "    unique_output_dir = Path(OUTPUT_PATH) / f'results_{image_name}'\n",
        "    unique_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # 모델 추론\n",
        "    results = model.predict(source=img_rgb, save=True, project=str(unique_output_dir), name='results')\n",
        "\n",
        "    # 결과 이미지 파일 경로 가져오기\n",
        "    output_images = list(unique_output_dir.glob('results/*.jpg'))  # 결과 이미지 찾기\n",
        "\n",
        "    if output_images:\n",
        "        output_image_path = str(output_images[0])  # 첫 번째 결과 이미지 선택\n",
        "    else:\n",
        "        print(f\"No output file found for {image_path}\")\n",
        "        return\n",
        "\n",
        "    # 결과 이미지 읽기 및 표시\n",
        "    output_img = cv2.imread(output_image_path)\n",
        "    if output_img is not None:\n",
        "        plt.imshow(cv2.cvtColor(output_img, cv2.COLOR_BGR2RGB))\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Failed to read the result image: {output_image_path}\")\n",
        "\n",
        "# 4. 테스트 데이터셋 평가 및 결과 출력\n",
        "image_paths = list(Path(TEST_DATASET_PATH).glob('*.*'))  # 이미지 파일 검색\n",
        "\n",
        "for image_path in image_paths:\n",
        "    print(f\"Processing {image_path}...\")\n",
        "    detect_and_plot(str(image_path), model)\n",
        "\n",
        "print(\"모든 이미지 평가 및 출력 완료.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv7meO7wWsS2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRUvL6jwK_AN",
        "outputId": "32237e22-0951-49fe-a4ad-765638c33b45"
      },
      "outputs": [],
      "source": [
        "# Macro F1 Score 계산을 위한 수정된 코드 셀\n",
        "\n",
        "# 필요한 라이브러리 임포트\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import yaml\n",
        "\n",
        "# IoU 계산 함수\n",
        "def compute_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    두 박스 간의 Intersection over Union (IoU)을 계산합니다.\n",
        "    박스는 [x1, y1, x2, y2] 형식이어야 합니다.\n",
        "    \"\"\"\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "\n",
        "    box1_area = max(0, (box1[2]-box1[0])) * max(0, (box1[3]-box1[1]))\n",
        "    box2_area = max(0, (box2[2]-box2[0])) * max(0, (box2[3]-box2[1]))\n",
        "\n",
        "    union_area = box1_area + box2_area - inter_area\n",
        "\n",
        "    if union_area == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return inter_area / union_area\n",
        "\n",
        "# 예측과 실제 라벨을 매칭하고 TP, FP, FN을 계산하는 함수\n",
        "def match_predictions(preds, targets, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    예측(preds)과 실제 객체(targets)를 매칭하여 클래스별 TP, FP, FN을 계산합니다.\n",
        "    \"\"\"\n",
        "    TP = defaultdict(int)\n",
        "    FP = defaultdict(int)\n",
        "    FN = defaultdict(int)\n",
        "\n",
        "    matched_targets = set()\n",
        "\n",
        "    for pred in preds:\n",
        "        pred_bbox = pred['bbox']\n",
        "        pred_class = pred['class']\n",
        "        best_iou = 0\n",
        "        best_target = None\n",
        "        for idx, target in enumerate(targets):\n",
        "            if target['class'] != pred_class or idx in matched_targets:\n",
        "                continue\n",
        "            iou = compute_iou(pred_bbox, target['bbox'])\n",
        "            if iou > best_iou:\n",
        "                best_iou = iou\n",
        "                best_target = idx\n",
        "        if best_iou >= iou_threshold:\n",
        "            TP[pred_class] += 1\n",
        "            matched_targets.add(best_target)\n",
        "        else:\n",
        "            FP[pred_class] += 1\n",
        "\n",
        "    for idx, target in enumerate(targets):\n",
        "        if idx not in matched_targets:\n",
        "            FN[target['class']] += 1\n",
        "\n",
        "    return TP, FP, FN\n",
        "\n",
        "# 데이터셋 및 모델 경로 설정\n",
        "MODEL_SAVE_DIR = \"custom_model_weights\"\n",
        "trained_weights_path = \"/content/custom_model_weights/ver5_yolov11_refined_best.pt\"  # 훈련된 모델 가중치 경로\n",
        "\n",
        "# 테스트 데이터셋 경로 (사용 중인 데이터셋 경로에 맞게 수정)\n",
        "dataset_path = \"/content/test-5\"  # 예: \"/content/dataset\"\n",
        "test_images_dir = os.path.join(dataset_path, \"test\", \"images\")\n",
        "test_labels_dir = os.path.join(dataset_path, \"test\", \"labels\")\n",
        "data_yaml_path = os.path.join(dataset_path, \"data.yaml\")\n",
        "\n",
        "# 클래스 이름 로드\n",
        "with open(data_yaml_path, \"r\") as yaml_file:\n",
        "    data_config = yaml.safe_load(yaml_file)\n",
        "class_names = data_config[\"names\"]\n",
        "\n",
        "# 모델 로드\n",
        "model = YOLO(trained_weights_path)\n",
        "\n",
        "# 메트릭 초기화\n",
        "metrics = defaultdict(lambda: {'TP': 0, 'FP': 0, 'FN': 0})\n",
        "\n",
        "# 테스트 이미지 파일 리스트\n",
        "test_image_files = [f for f in os.listdir(test_images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "# 모든 테스트 이미지에 대해 예측 및 메트릭 계산\n",
        "for image_file in tqdm(test_image_files, desc=\"테스트 이미지 처리 중\"):\n",
        "    image_path = os.path.join(test_images_dir, image_file)\n",
        "    label_file = os.path.splitext(image_file)[0] + \".txt\"\n",
        "    label_path = os.path.join(test_labels_dir, label_file)\n",
        "\n",
        "    # 실제 객체 로드\n",
        "    targets = []\n",
        "    if os.path.exists(label_path):\n",
        "        with open(label_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        for line in lines:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 5:\n",
        "                print(f\"라벨 파일 형식 오류: {label_path}\")\n",
        "                continue\n",
        "            class_id = int(parts[0])\n",
        "            x_center, y_center, w, h = map(float, parts[1:5])\n",
        "            img = cv2.imread(image_path)\n",
        "            if img is None:\n",
        "                print(f\"이미지 로드 실패: {image_path}\")\n",
        "                continue\n",
        "            height, width, _ = img.shape\n",
        "            x1 = max(0, (x_center - w/2) * width)\n",
        "            y1 = max(0, (y_center - h/2) * height)\n",
        "            x2 = min(width, (x_center + w/2) * width)\n",
        "            y2 = min(height, (y_center + h/2) * height)\n",
        "            targets.append({\n",
        "                'bbox': [x1, y1, x2, y2],\n",
        "                'class': class_id\n",
        "            })\n",
        "\n",
        "    # 예측 수행\n",
        "    results = model(image_path, verbose=False)\n",
        "    preds = []\n",
        "    for result in results:\n",
        "        boxes = result.boxes\n",
        "        for box in boxes:\n",
        "            cls = int(box.cls.cpu().numpy().item())  # 수정된 부분\n",
        "            conf = float(box.conf.cpu().numpy().item())  # 수정된 부분\n",
        "            xyxy = box.xyxy.cpu().numpy().flatten()\n",
        "            preds.append({\n",
        "                'bbox': xyxy.tolist(),  # [x1, y1, x2, y2]\n",
        "                'class': cls,\n",
        "                'score': conf\n",
        "            })\n",
        "\n",
        "    # 예측과 실제 객체 매칭\n",
        "    TP, FP, FN = match_predictions(preds, targets, iou_threshold=0.5)\n",
        "\n",
        "    # 메트릭 업데이트\n",
        "    for cls in TP:\n",
        "        metrics[cls]['TP'] += TP[cls]\n",
        "    for cls in FP:\n",
        "        metrics[cls]['FP'] += FP[cls]\n",
        "    for cls in FN:\n",
        "        metrics[cls]['FN'] += FN[cls]\n",
        "\n",
        "# 클래스별 F1 Score 및 Macro F1 Score 계산\n",
        "f1_scores = {}\n",
        "for cls in metrics:\n",
        "    TP = metrics[cls]['TP']\n",
        "    FP = metrics[cls]['FP']\n",
        "    FN = metrics[cls]['FN']\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    f1_scores[cls] = f1\n",
        "    print(f\"클래스 {cls} ({class_names[cls]}): Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}\")\n",
        "\n",
        "# Macro F1 Score 계산\n",
        "if f1_scores:\n",
        "    macro_f1 = np.mean(list(f1_scores.values()))\n",
        "    print(f\"\\nMacro F1 Score: {macro_f1:.4f}\")\n",
        "else:\n",
        "    print(\"클래스별 F1 Score를 계산할 데이터가 없습니다.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ukW18IrdaRV",
        "outputId": "7e8e177c-8576-4c62-ec9e-129cbd6c0c08"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 클래스별 정밀도, 재현율, F1 점수 입력\n",
        "class_precision = [0.9568, 0.9808, 0.9807, 0.9893, 0.9924, 0.9984, 0.9659, 0.9846, 0.9738, 0.9929]\n",
        "class_recall = [1.0000, 0.8667, 0.9643, 1.0000, 1.0000, 1.0000, 0.9231, 0.9935, 0.9545, 0.9941]\n",
        "\n",
        "# F1 점수를 계산하는 함수\n",
        "def calculate_f1(precision, recall):\n",
        "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "# 클래스별 F1 점수 계산\n",
        "class_f1 = [calculate_f1(p, r) for p, r in zip(class_precision, class_recall)]\n",
        "\n",
        "# Macro average 계산\n",
        "macro_avg_precision = np.mean(class_precision)\n",
        "macro_avg_recall = np.mean(class_recall)\n",
        "macro_avg_f1 = np.mean(class_f1)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"Macro Average Precision: {:.4f}\".format(macro_avg_precision))\n",
        "print(\"Macro Average Recall: {:.4f}\".format(macro_avg_recall))\n",
        "print(\"Macro Average F1 Score: {:.4f}\".format(macro_avg_f1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cC6Wlch9aDkO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
